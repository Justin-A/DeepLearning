{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "assert tf.__version__.startswith(\"2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "x_train, x_test = np.expand_dims(x_train, axis = 3), np.expand_dims(x_test, axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(256)\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,)\n",
      "(10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(keras.Model):\n",
    "    def __init__(self, ch, kernelsz = 3, strides = 1, padding = \"same\"):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        \n",
    "        self.model = keras.models.Sequential([\n",
    "            keras.layers.Conv2D(ch, kernelsz, strides = strides, padding = padding),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.ReLU()\n",
    "        ])\n",
    "    \n",
    "    def call(self, x, training = None):\n",
    "        x = self.model(x, training = training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlk(keras.Model):\n",
    "    def __init__(self, ch, strides = 1):\n",
    "        super(InceptionBlk, self).__init__()\n",
    "        self.ch = ch\n",
    "        self.strides = strides\n",
    "        \n",
    "        self.conv1 = ConvBNReLU(ch, strides = strides)\n",
    "        self.conv2 = ConvBNReLU(ch, kernelsz = 3, strides = strides)\n",
    "        self.conv3_1 = ConvBNReLU(ch, kernelsz = 3, strides = strides)\n",
    "        self.conv3_2 = ConvBNReLU(ch, kernelsz = 3, strides = 1)\n",
    "        \n",
    "        self.pool = keras.layers.MaxPooling2D(3, strides = 1, padding = \"same\")\n",
    "        self.pool_conv = ConvBNReLU(ch, strides = strides)\n",
    "        \n",
    "    def call(self, x, training = None):\n",
    "        x1 = self.conv1(x, training = training)\n",
    "        x2 = self.conv2(x, training = training)\n",
    "        x3_1 = self.conv3_1(x, training = training)\n",
    "        x3_2 = self.conv3_2(x3_1, training = training)\n",
    "        x4 = self.pool(x)\n",
    "        x4 = self.pool_conv(x4, training = training)\n",
    "        x = tf.concat([x1, x2, x3_2, x4], axis = 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(keras.Model):\n",
    "    def __init__(self, num_layers, num_classes, init_ch = 16, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        self.in_channels = init_ch\n",
    "        self.out_channels = init_ch\n",
    "        self.num_layers = num_layers\n",
    "        self.init_ch = init_ch\n",
    "        \n",
    "        self.conv1 = ConvBNReLU(init_ch)\n",
    "        self.blocks = keras.models.Sequential(name = \"DYNAMIC-BLOCKS\")\n",
    "        for block_id in range(num_layers):\n",
    "            for layer_id in range(2):\n",
    "                if layer_id == 0:\n",
    "                    block = InceptionBlk(self.out_channels, strides = 2)\n",
    "                else:\n",
    "                    block = InceptionBlk(self.out_channels, strides = 1)\n",
    "                    \n",
    "                self.blocks.add(block)\n",
    "            self.out_channels *= 2\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.fc = keras.layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, x, training = None):\n",
    "        out = self.conv1(x, training = training)\n",
    "        out = self.blocks(out, training = training)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_bn_re_lu (ConvBNReLU)   multiple                  224       \n",
      "_________________________________________________________________\n",
      "DYNAMIC-BLOCKS (Sequential)  multiple                  292704    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 294,218\n",
      "Trainable params: 293,226\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "model = Inception(2, 10)\n",
    "model.build(input_shape = (None, 28, 28, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "criterion = keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "acc_meter = keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Loss:  2.3067923\n",
      "Step:  10 Loss:  2.104604\n",
      "Step:  20 Loss:  1.5719802\n",
      "Step:  30 Loss:  1.3793477\n",
      "Step:  40 Loss:  0.9467964\n",
      "Step:  50 Loss:  0.8574427\n",
      "Step:  60 Loss:  0.58914363\n",
      "Step:  70 Loss:  0.35742673\n",
      "Step:  80 Loss:  0.5373102\n",
      "Step:  90 Loss:  0.4555395\n",
      "Step:  100 Loss:  0.38353968\n",
      "Step:  110 Loss:  0.34024346\n",
      "Step:  120 Loss:  0.34489682\n",
      "Step:  130 Loss:  0.20710012\n",
      "Step:  140 Loss:  0.25235802\n",
      "Step:  150 Loss:  0.21698198\n",
      "Step:  160 Loss:  0.2321302\n",
      "Step:  170 Loss:  0.18356465\n",
      "Step:  180 Loss:  0.3122157\n",
      "Step:  190 Loss:  0.13903624\n",
      "Step:  200 Loss:  0.2680899\n",
      "Step:  210 Loss:  0.18700425\n",
      "Step:  220 Loss:  0.1676037\n",
      "Step:  230 Loss:  0.03278955\n",
      "Epochs:  0 Evaluation Accuracy:  0.9546\n",
      "Step:  0 Loss:  0.17778103\n",
      "Step:  10 Loss:  0.15723905\n",
      "Step:  20 Loss:  0.21589167\n",
      "Step:  30 Loss:  0.2675207\n",
      "Step:  40 Loss:  0.3387717\n",
      "Step:  50 Loss:  0.3005675\n",
      "Step:  60 Loss:  0.2221539\n",
      "Step:  70 Loss:  0.12473531\n",
      "Step:  80 Loss:  0.14233962\n",
      "Step:  90 Loss:  0.12571849\n",
      "Step:  100 Loss:  0.1531111\n",
      "Step:  110 Loss:  0.20934743\n",
      "Step:  120 Loss:  0.11361546\n",
      "Step:  130 Loss:  0.12578288\n",
      "Step:  140 Loss:  0.12329631\n",
      "Step:  150 Loss:  0.09640894\n",
      "Step:  160 Loss:  0.11145958\n",
      "Step:  170 Loss:  0.08420938\n",
      "Step:  180 Loss:  0.16472144\n",
      "Step:  190 Loss:  0.059986036\n",
      "Step:  200 Loss:  0.13699088\n",
      "Step:  210 Loss:  0.1211106\n",
      "Step:  220 Loss:  0.17836377\n",
      "Step:  230 Loss:  0.00700894\n",
      "Epochs:  1 Evaluation Accuracy:  0.9652\n",
      "Step:  0 Loss:  0.11892187\n",
      "Step:  10 Loss:  0.123344384\n",
      "Step:  20 Loss:  0.123392016\n",
      "Step:  30 Loss:  0.15219384\n",
      "Step:  40 Loss:  0.15251212\n",
      "Step:  50 Loss:  0.14012219\n",
      "Step:  60 Loss:  0.24942973\n",
      "Step:  70 Loss:  0.11957167\n",
      "Step:  80 Loss:  0.1051483\n",
      "Step:  90 Loss:  0.07961483\n",
      "Step:  100 Loss:  0.07905828\n",
      "Step:  110 Loss:  0.11168698\n",
      "Step:  120 Loss:  0.0721069\n",
      "Step:  130 Loss:  0.13527285\n",
      "Step:  140 Loss:  0.07187191\n",
      "Step:  150 Loss:  0.13459074\n",
      "Step:  160 Loss:  0.0784449\n",
      "Step:  170 Loss:  0.0699413\n",
      "Step:  180 Loss:  0.11402901\n",
      "Step:  190 Loss:  0.049325846\n",
      "Step:  200 Loss:  0.08479962\n",
      "Step:  210 Loss:  0.09013978\n",
      "Step:  220 Loss:  0.10639566\n",
      "Step:  230 Loss:  0.00806763\n",
      "Epochs:  2 Evaluation Accuracy:  0.9767\n",
      "Step:  0 Loss:  0.0812505\n",
      "Step:  10 Loss:  0.13673313\n",
      "Step:  20 Loss:  0.10034105\n",
      "Step:  30 Loss:  0.121993214\n",
      "Step:  40 Loss:  0.08937558\n",
      "Step:  50 Loss:  0.050183475\n",
      "Step:  60 Loss:  0.12726486\n",
      "Step:  70 Loss:  0.081890404\n",
      "Step:  80 Loss:  0.08960742\n",
      "Step:  90 Loss:  0.05691187\n",
      "Step:  100 Loss:  0.05719295\n",
      "Step:  110 Loss:  0.06670103\n",
      "Step:  120 Loss:  0.054278843\n",
      "Step:  130 Loss:  0.11928848\n",
      "Step:  140 Loss:  0.05884771\n",
      "Step:  150 Loss:  0.101601176\n",
      "Step:  160 Loss:  0.056639455\n",
      "Step:  170 Loss:  0.07419785\n",
      "Step:  180 Loss:  0.07119354\n",
      "Step:  190 Loss:  0.025414899\n",
      "Step:  200 Loss:  0.06610548\n",
      "Step:  210 Loss:  0.075837895\n",
      "Step:  220 Loss:  0.09173574\n",
      "Step:  230 Loss:  0.0063360934\n",
      "Epochs:  3 Evaluation Accuracy:  0.974\n",
      "Step:  0 Loss:  0.07808744\n",
      "Step:  10 Loss:  0.08219312\n",
      "Step:  20 Loss:  0.056453783\n",
      "Step:  30 Loss:  0.0785565\n",
      "Step:  40 Loss:  0.06595013\n",
      "Step:  50 Loss:  0.053689353\n",
      "Step:  60 Loss:  0.10923646\n",
      "Step:  70 Loss:  0.07975042\n",
      "Step:  80 Loss:  0.0838155\n",
      "Step:  90 Loss:  0.034344286\n",
      "Step:  100 Loss:  0.046428446\n",
      "Step:  110 Loss:  0.03372454\n",
      "Step:  120 Loss:  0.04677748\n",
      "Step:  130 Loss:  0.08826977\n",
      "Step:  140 Loss:  0.055610936\n",
      "Step:  150 Loss:  0.071009725\n",
      "Step:  160 Loss:  0.06329587\n",
      "Step:  170 Loss:  0.058242578\n",
      "Step:  180 Loss:  0.058972575\n",
      "Step:  190 Loss:  0.01902211\n",
      "Step:  200 Loss:  0.05311726\n",
      "Step:  210 Loss:  0.07403967\n",
      "Step:  220 Loss:  0.08379237\n",
      "Step:  230 Loss:  0.0071756267\n",
      "Epochs:  4 Evaluation Accuracy:  0.9769\n",
      "Step:  0 Loss:  0.065520495\n",
      "Step:  10 Loss:  0.09284575\n",
      "Step:  20 Loss:  0.05145232\n",
      "Step:  30 Loss:  0.07960773\n",
      "Step:  40 Loss:  0.034346167\n",
      "Step:  50 Loss:  0.03791585\n",
      "Step:  60 Loss:  0.101145126\n",
      "Step:  70 Loss:  0.0737495\n",
      "Step:  80 Loss:  0.06764145\n",
      "Step:  90 Loss:  0.023536565\n",
      "Step:  100 Loss:  0.04615466\n",
      "Step:  110 Loss:  0.028946826\n",
      "Step:  120 Loss:  0.04993905\n",
      "Step:  130 Loss:  0.06532436\n",
      "Step:  140 Loss:  0.04738342\n",
      "Step:  150 Loss:  0.053189293\n",
      "Step:  160 Loss:  0.056223627\n",
      "Step:  170 Loss:  0.0461713\n",
      "Step:  180 Loss:  0.061617337\n",
      "Step:  190 Loss:  0.012827273\n",
      "Step:  200 Loss:  0.047558337\n",
      "Step:  210 Loss:  0.051256336\n",
      "Step:  220 Loss:  0.08702685\n",
      "Step:  230 Loss:  0.0036157998\n",
      "Epochs:  5 Evaluation Accuracy:  0.9881\n",
      "Step:  0 Loss:  0.03408686\n",
      "Step:  10 Loss:  0.09968647\n",
      "Step:  20 Loss:  0.03443255\n",
      "Step:  30 Loss:  0.07071912\n",
      "Step:  40 Loss:  0.01970872\n",
      "Step:  50 Loss:  0.028448645\n",
      "Step:  60 Loss:  0.07566914\n",
      "Step:  70 Loss:  0.064449295\n",
      "Step:  80 Loss:  0.06569734\n",
      "Step:  90 Loss:  0.01752332\n",
      "Step:  100 Loss:  0.040967226\n",
      "Step:  110 Loss:  0.018574588\n",
      "Step:  120 Loss:  0.04296541\n",
      "Step:  130 Loss:  0.051734902\n",
      "Step:  140 Loss:  0.03611107\n",
      "Step:  150 Loss:  0.040841855\n",
      "Step:  160 Loss:  0.051383942\n",
      "Step:  170 Loss:  0.033484895\n",
      "Step:  180 Loss:  0.040413603\n",
      "Step:  190 Loss:  0.009590056\n",
      "Step:  200 Loss:  0.044155873\n",
      "Step:  210 Loss:  0.05232402\n",
      "Step:  220 Loss:  0.04245735\n",
      "Step:  230 Loss:  0.0005862498\n",
      "Epochs:  6 Evaluation Accuracy:  0.9898\n",
      "Step:  0 Loss:  0.023108412\n",
      "Step:  10 Loss:  0.08534251\n",
      "Step:  20 Loss:  0.025858892\n",
      "Step:  30 Loss:  0.057334162\n",
      "Step:  40 Loss:  0.01968703\n",
      "Step:  50 Loss:  0.025233295\n",
      "Step:  60 Loss:  0.050285522\n",
      "Step:  70 Loss:  0.058913127\n",
      "Step:  80 Loss:  0.0638977\n",
      "Step:  90 Loss:  0.008458346\n",
      "Step:  100 Loss:  0.04117296\n",
      "Step:  110 Loss:  0.013004698\n",
      "Step:  120 Loss:  0.04210976\n",
      "Step:  130 Loss:  0.045110293\n",
      "Step:  140 Loss:  0.025624719\n",
      "Step:  150 Loss:  0.039231513\n",
      "Step:  160 Loss:  0.045463264\n",
      "Step:  170 Loss:  0.024090994\n",
      "Step:  180 Loss:  0.027321476\n",
      "Step:  190 Loss:  0.0065624802\n",
      "Step:  200 Loss:  0.036525603\n",
      "Step:  210 Loss:  0.041327335\n",
      "Step:  220 Loss:  0.027995704\n",
      "Step:  230 Loss:  0.00016770138\n",
      "Epochs:  7 Evaluation Accuracy:  0.9903\n",
      "Step:  0 Loss:  0.01842934\n",
      "Step:  10 Loss:  0.08444886\n",
      "Step:  20 Loss:  0.022032846\n",
      "Step:  30 Loss:  0.04582997\n",
      "Step:  40 Loss:  0.051886905\n",
      "Step:  50 Loss:  0.033292856\n",
      "Step:  60 Loss:  0.05801919\n",
      "Step:  70 Loss:  0.049861178\n",
      "Step:  80 Loss:  0.06300175\n",
      "Step:  90 Loss:  0.007999946\n",
      "Step:  100 Loss:  0.027997982\n",
      "Step:  110 Loss:  0.04287168\n",
      "Step:  120 Loss:  0.018246625\n",
      "Step:  130 Loss:  0.05042182\n",
      "Step:  140 Loss:  0.03703555\n",
      "Step:  150 Loss:  0.031766333\n",
      "Step:  160 Loss:  0.053012617\n",
      "Step:  170 Loss:  0.022321554\n",
      "Step:  180 Loss:  0.031974263\n",
      "Step:  190 Loss:  0.0068472885\n",
      "Step:  200 Loss:  0.034524262\n",
      "Step:  210 Loss:  0.03295318\n",
      "Step:  220 Loss:  0.025937688\n",
      "Step:  230 Loss:  0.00035427435\n",
      "Epochs:  8 Evaluation Accuracy:  0.9889\n",
      "Step:  0 Loss:  0.023903932\n",
      "Step:  10 Loss:  0.07145699\n",
      "Step:  20 Loss:  0.023951015\n",
      "Step:  30 Loss:  0.021563329\n",
      "Step:  40 Loss:  0.047576472\n",
      "Step:  50 Loss:  0.030118478\n",
      "Step:  60 Loss:  0.049233366\n",
      "Step:  70 Loss:  0.05550883\n",
      "Step:  80 Loss:  0.05064941\n",
      "Step:  90 Loss:  0.004533598\n",
      "Step:  100 Loss:  0.034430098\n",
      "Step:  110 Loss:  0.01661298\n",
      "Step:  120 Loss:  0.0302152\n",
      "Step:  130 Loss:  0.039350692\n",
      "Step:  140 Loss:  0.048967835\n",
      "Step:  150 Loss:  0.028757188\n",
      "Step:  160 Loss:  0.04525435\n",
      "Step:  170 Loss:  0.02074065\n",
      "Step:  180 Loss:  0.035116635\n",
      "Step:  190 Loss:  0.0090653775\n",
      "Step:  200 Loss:  0.02591607\n",
      "Step:  210 Loss:  0.034181286\n",
      "Step:  220 Loss:  0.017297195\n",
      "Step:  230 Loss:  0.00018998084\n",
      "Epochs:  9 Evaluation Accuracy:  0.9909\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y) in enumerate(db_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss = criterion(tf.one_hot(y, depth = 10), logits)\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(\"Step: \", step, \"Loss: \", loss.numpy())\n",
    "    \n",
    "    acc_meter.reset_states()\n",
    "    for x, y in db_test:\n",
    "        logits = model(x, training = False)\n",
    "        pred = tf.argmax(logits, axis = 1)\n",
    "        acc_meter.update_state(y, pred)\n",
    "    \n",
    "    print(\"Epochs: \", epoch, \"Evaluation Accuracy: \", acc_meter.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
