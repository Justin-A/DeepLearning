{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "BATCH_SIZE = 100\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST('./.data', train = True, download = True, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))]))\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        c = self.embed(labels)\n",
    "        x = torch.cat([z, c] , 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        c = self.embed(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator().to(DEVICE)\n",
    "G = Generator().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(D.parameters(), lr = 0.0002)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH [0/300] D_Loss:0.2403 G_Loss: 4.1071 D(x):0.91 D(G(z)):0.10\n",
      "EPOCH [1/300] D_Loss:0.3733 G_Loss: 3.8627 D(x):0.89 D(G(z)):0.13\n",
      "EPOCH [2/300] D_Loss:0.2325 G_Loss: 4.7050 D(x):0.93 D(G(z)):0.09\n",
      "EPOCH [3/300] D_Loss:0.3046 G_Loss: 4.0621 D(x):0.91 D(G(z)):0.10\n",
      "EPOCH [4/300] D_Loss:0.2753 G_Loss: 4.3986 D(x):0.87 D(G(z)):0.04\n",
      "EPOCH [5/300] D_Loss:0.3834 G_Loss: 2.8047 D(x):0.89 D(G(z)):0.15\n",
      "EPOCH [6/300] D_Loss:0.4950 G_Loss: 2.5191 D(x):0.85 D(G(z)):0.19\n",
      "EPOCH [7/300] D_Loss:0.7559 G_Loss: 2.7569 D(x):0.79 D(G(z)):0.18\n",
      "EPOCH [8/300] D_Loss:0.6375 G_Loss: 2.3447 D(x):0.78 D(G(z)):0.16\n",
      "EPOCH [9/300] D_Loss:0.6730 G_Loss: 2.2048 D(x):0.88 D(G(z)):0.27\n",
      "EPOCH [10/300] D_Loss:0.5994 G_Loss: 2.5295 D(x):0.79 D(G(z)):0.15\n",
      "EPOCH [11/300] D_Loss:1.1211 G_Loss: 1.5002 D(x):0.67 D(G(z)):0.33\n",
      "EPOCH [12/300] D_Loss:0.9682 G_Loss: 1.6342 D(x):0.73 D(G(z)):0.30\n",
      "EPOCH [13/300] D_Loss:0.7677 G_Loss: 1.6688 D(x):0.73 D(G(z)):0.25\n",
      "EPOCH [14/300] D_Loss:0.8907 G_Loss: 1.7458 D(x):0.69 D(G(z)):0.28\n",
      "EPOCH [15/300] D_Loss:0.8333 G_Loss: 1.4472 D(x):0.71 D(G(z)):0.31\n",
      "EPOCH [16/300] D_Loss:0.8909 G_Loss: 1.3796 D(x):0.71 D(G(z)):0.32\n",
      "EPOCH [17/300] D_Loss:0.8586 G_Loss: 1.3593 D(x):0.76 D(G(z)):0.33\n",
      "EPOCH [18/300] D_Loss:0.8045 G_Loss: 1.5740 D(x):0.73 D(G(z)):0.27\n",
      "EPOCH [19/300] D_Loss:0.9047 G_Loss: 1.3253 D(x):0.72 D(G(z)):0.32\n",
      "EPOCH [20/300] D_Loss:0.9842 G_Loss: 1.2856 D(x):0.69 D(G(z)):0.34\n",
      "EPOCH [21/300] D_Loss:1.0976 G_Loss: 1.2142 D(x):0.72 D(G(z)):0.42\n",
      "EPOCH [22/300] D_Loss:1.1583 G_Loss: 0.9763 D(x):0.60 D(G(z)):0.42\n",
      "EPOCH [23/300] D_Loss:0.6581 G_Loss: 1.6010 D(x):0.77 D(G(z)):0.25\n",
      "EPOCH [24/300] D_Loss:0.9523 G_Loss: 1.6532 D(x):0.71 D(G(z)):0.33\n",
      "EPOCH [25/300] D_Loss:0.9856 G_Loss: 1.1875 D(x):0.69 D(G(z)):0.37\n",
      "EPOCH [26/300] D_Loss:1.1402 G_Loss: 1.2558 D(x):0.64 D(G(z)):0.37\n",
      "EPOCH [27/300] D_Loss:0.9606 G_Loss: 1.4858 D(x):0.69 D(G(z)):0.32\n",
      "EPOCH [28/300] D_Loss:1.0059 G_Loss: 1.4163 D(x):0.62 D(G(z)):0.32\n",
      "EPOCH [29/300] D_Loss:0.9003 G_Loss: 1.3250 D(x):0.71 D(G(z)):0.33\n",
      "EPOCH [30/300] D_Loss:1.0638 G_Loss: 1.5555 D(x):0.61 D(G(z)):0.30\n",
      "EPOCH [31/300] D_Loss:0.9621 G_Loss: 1.2902 D(x):0.66 D(G(z)):0.34\n",
      "EPOCH [32/300] D_Loss:1.0013 G_Loss: 1.2223 D(x):0.67 D(G(z)):0.36\n",
      "EPOCH [33/300] D_Loss:1.1365 G_Loss: 1.1425 D(x):0.60 D(G(z)):0.38\n",
      "EPOCH [34/300] D_Loss:0.8543 G_Loss: 1.2277 D(x):0.74 D(G(z)):0.33\n",
      "EPOCH [35/300] D_Loss:0.9003 G_Loss: 1.2664 D(x):0.73 D(G(z)):0.34\n",
      "EPOCH [36/300] D_Loss:1.0396 G_Loss: 1.2580 D(x):0.62 D(G(z)):0.33\n",
      "EPOCH [37/300] D_Loss:1.1404 G_Loss: 1.0468 D(x):0.59 D(G(z)):0.38\n",
      "EPOCH [38/300] D_Loss:1.1006 G_Loss: 1.0731 D(x):0.65 D(G(z)):0.40\n",
      "EPOCH [39/300] D_Loss:1.1393 G_Loss: 1.0919 D(x):0.59 D(G(z)):0.37\n",
      "EPOCH [40/300] D_Loss:1.0712 G_Loss: 1.1977 D(x):0.66 D(G(z)):0.38\n",
      "EPOCH [41/300] D_Loss:0.8722 G_Loss: 1.3994 D(x):0.68 D(G(z)):0.29\n",
      "EPOCH [42/300] D_Loss:1.0903 G_Loss: 1.3185 D(x):0.63 D(G(z)):0.34\n",
      "EPOCH [43/300] D_Loss:1.2638 G_Loss: 0.9345 D(x):0.52 D(G(z)):0.38\n",
      "EPOCH [44/300] D_Loss:1.1567 G_Loss: 0.9638 D(x):0.61 D(G(z)):0.42\n",
      "EPOCH [45/300] D_Loss:0.9821 G_Loss: 1.1621 D(x):0.66 D(G(z)):0.36\n",
      "EPOCH [46/300] D_Loss:1.1234 G_Loss: 1.0298 D(x):0.58 D(G(z)):0.37\n",
      "EPOCH [47/300] D_Loss:1.1023 G_Loss: 1.1823 D(x):0.62 D(G(z)):0.37\n",
      "EPOCH [48/300] D_Loss:1.1014 G_Loss: 1.1858 D(x):0.64 D(G(z)):0.39\n",
      "EPOCH [49/300] D_Loss:1.2534 G_Loss: 1.0638 D(x):0.60 D(G(z)):0.41\n",
      "EPOCH [50/300] D_Loss:1.1833 G_Loss: 0.9838 D(x):0.59 D(G(z)):0.41\n",
      "EPOCH [51/300] D_Loss:1.1621 G_Loss: 1.1486 D(x):0.63 D(G(z)):0.38\n",
      "EPOCH [52/300] D_Loss:1.0595 G_Loss: 1.0657 D(x):0.65 D(G(z)):0.39\n",
      "EPOCH [53/300] D_Loss:1.1230 G_Loss: 1.0351 D(x):0.63 D(G(z)):0.41\n",
      "EPOCH [54/300] D_Loss:0.9594 G_Loss: 1.3520 D(x):0.69 D(G(z)):0.31\n",
      "EPOCH [55/300] D_Loss:1.1141 G_Loss: 1.1489 D(x):0.61 D(G(z)):0.37\n",
      "EPOCH [56/300] D_Loss:1.1644 G_Loss: 1.0100 D(x):0.66 D(G(z)):0.43\n",
      "EPOCH [57/300] D_Loss:1.2224 G_Loss: 1.2583 D(x):0.69 D(G(z)):0.39\n",
      "EPOCH [58/300] D_Loss:1.0693 G_Loss: 1.1458 D(x):0.70 D(G(z)):0.38\n",
      "EPOCH [59/300] D_Loss:1.0804 G_Loss: 1.0249 D(x):0.67 D(G(z)):0.42\n",
      "EPOCH [60/300] D_Loss:1.1758 G_Loss: 1.1071 D(x):0.61 D(G(z)):0.41\n",
      "EPOCH [61/300] D_Loss:1.1964 G_Loss: 1.0842 D(x):0.60 D(G(z)):0.40\n",
      "EPOCH [62/300] D_Loss:1.1335 G_Loss: 1.3155 D(x):0.63 D(G(z)):0.36\n",
      "EPOCH [63/300] D_Loss:1.1380 G_Loss: 1.1218 D(x):0.63 D(G(z)):0.39\n",
      "EPOCH [64/300] D_Loss:0.9671 G_Loss: 1.1450 D(x):0.66 D(G(z)):0.37\n",
      "EPOCH [65/300] D_Loss:1.0796 G_Loss: 1.0218 D(x):0.66 D(G(z)):0.41\n",
      "EPOCH [66/300] D_Loss:1.2174 G_Loss: 1.0040 D(x):0.59 D(G(z)):0.42\n",
      "EPOCH [67/300] D_Loss:1.1010 G_Loss: 1.1614 D(x):0.67 D(G(z)):0.41\n",
      "EPOCH [68/300] D_Loss:0.9971 G_Loss: 1.1861 D(x):0.68 D(G(z)):0.39\n",
      "EPOCH [69/300] D_Loss:1.0361 G_Loss: 1.2895 D(x):0.64 D(G(z)):0.35\n",
      "EPOCH [70/300] D_Loss:1.1081 G_Loss: 1.0317 D(x):0.60 D(G(z)):0.38\n",
      "EPOCH [71/300] D_Loss:1.2338 G_Loss: 0.9861 D(x):0.64 D(G(z)):0.43\n",
      "EPOCH [72/300] D_Loss:1.2639 G_Loss: 1.0753 D(x):0.58 D(G(z)):0.41\n",
      "EPOCH [73/300] D_Loss:1.2134 G_Loss: 0.9813 D(x):0.57 D(G(z)):0.39\n",
      "EPOCH [74/300] D_Loss:1.0955 G_Loss: 1.0552 D(x):0.62 D(G(z)):0.36\n",
      "EPOCH [75/300] D_Loss:1.2731 G_Loss: 0.8089 D(x):0.59 D(G(z)):0.47\n",
      "EPOCH [76/300] D_Loss:1.2258 G_Loss: 0.9625 D(x):0.62 D(G(z)):0.42\n",
      "EPOCH [77/300] D_Loss:1.1323 G_Loss: 1.0743 D(x):0.62 D(G(z)):0.39\n",
      "EPOCH [78/300] D_Loss:1.2666 G_Loss: 1.3409 D(x):0.59 D(G(z)):0.38\n",
      "EPOCH [79/300] D_Loss:1.0735 G_Loss: 1.0736 D(x):0.62 D(G(z)):0.36\n",
      "EPOCH [80/300] D_Loss:1.2357 G_Loss: 1.0693 D(x):0.59 D(G(z)):0.39\n",
      "EPOCH [81/300] D_Loss:1.1888 G_Loss: 1.1892 D(x):0.61 D(G(z)):0.39\n",
      "EPOCH [82/300] D_Loss:1.3847 G_Loss: 1.2328 D(x):0.51 D(G(z)):0.41\n",
      "EPOCH [83/300] D_Loss:1.2641 G_Loss: 1.1343 D(x):0.60 D(G(z)):0.42\n",
      "EPOCH [84/300] D_Loss:1.1468 G_Loss: 0.9777 D(x):0.65 D(G(z)):0.45\n",
      "EPOCH [85/300] D_Loss:1.0345 G_Loss: 1.1705 D(x):0.65 D(G(z)):0.37\n",
      "EPOCH [86/300] D_Loss:1.2292 G_Loss: 1.3047 D(x):0.54 D(G(z)):0.33\n",
      "EPOCH [87/300] D_Loss:1.2185 G_Loss: 1.2040 D(x):0.55 D(G(z)):0.36\n",
      "EPOCH [88/300] D_Loss:1.0934 G_Loss: 1.1898 D(x):0.62 D(G(z)):0.36\n",
      "EPOCH [89/300] D_Loss:1.3754 G_Loss: 1.0106 D(x):0.51 D(G(z)):0.41\n",
      "EPOCH [90/300] D_Loss:1.2231 G_Loss: 1.3144 D(x):0.65 D(G(z)):0.41\n",
      "EPOCH [91/300] D_Loss:1.3901 G_Loss: 0.9809 D(x):0.55 D(G(z)):0.43\n",
      "EPOCH [92/300] D_Loss:1.2234 G_Loss: 0.8943 D(x):0.59 D(G(z)):0.45\n",
      "EPOCH [93/300] D_Loss:1.1252 G_Loss: 1.1904 D(x):0.60 D(G(z)):0.37\n",
      "EPOCH [94/300] D_Loss:1.3152 G_Loss: 0.9790 D(x):0.52 D(G(z)):0.41\n",
      "EPOCH [95/300] D_Loss:1.1530 G_Loss: 1.0622 D(x):0.59 D(G(z)):0.39\n",
      "EPOCH [96/300] D_Loss:0.9218 G_Loss: 1.2203 D(x):0.67 D(G(z)):0.33\n",
      "EPOCH [97/300] D_Loss:1.1160 G_Loss: 1.1232 D(x):0.58 D(G(z)):0.37\n",
      "EPOCH [98/300] D_Loss:1.2069 G_Loss: 1.3513 D(x):0.63 D(G(z)):0.38\n",
      "EPOCH [99/300] D_Loss:1.1390 G_Loss: 1.0544 D(x):0.65 D(G(z)):0.43\n",
      "EPOCH [100/300] D_Loss:1.1452 G_Loss: 0.9560 D(x):0.63 D(G(z)):0.44\n",
      "EPOCH [101/300] D_Loss:1.1899 G_Loss: 1.0808 D(x):0.62 D(G(z)):0.42\n",
      "EPOCH [102/300] D_Loss:1.1484 G_Loss: 0.9603 D(x):0.62 D(G(z)):0.41\n",
      "EPOCH [103/300] D_Loss:1.2331 G_Loss: 1.0309 D(x):0.58 D(G(z)):0.40\n",
      "EPOCH [104/300] D_Loss:1.2370 G_Loss: 0.9406 D(x):0.60 D(G(z)):0.46\n",
      "EPOCH [105/300] D_Loss:1.2413 G_Loss: 0.8963 D(x):0.56 D(G(z)):0.44\n",
      "EPOCH [106/300] D_Loss:1.1480 G_Loss: 1.0396 D(x):0.61 D(G(z)):0.40\n",
      "EPOCH [107/300] D_Loss:1.1458 G_Loss: 1.0203 D(x):0.59 D(G(z)):0.39\n",
      "EPOCH [108/300] D_Loss:1.2500 G_Loss: 1.0310 D(x):0.53 D(G(z)):0.40\n",
      "EPOCH [109/300] D_Loss:1.2161 G_Loss: 1.0340 D(x):0.56 D(G(z)):0.40\n",
      "EPOCH [110/300] D_Loss:1.2111 G_Loss: 1.0266 D(x):0.60 D(G(z)):0.42\n",
      "EPOCH [111/300] D_Loss:1.1936 G_Loss: 0.9425 D(x):0.62 D(G(z)):0.44\n",
      "EPOCH [112/300] D_Loss:1.1578 G_Loss: 0.9962 D(x):0.57 D(G(z)):0.39\n",
      "EPOCH [113/300] D_Loss:1.1904 G_Loss: 0.9734 D(x):0.57 D(G(z)):0.41\n",
      "EPOCH [114/300] D_Loss:1.2466 G_Loss: 0.8421 D(x):0.58 D(G(z)):0.46\n",
      "EPOCH [115/300] D_Loss:1.2906 G_Loss: 0.8768 D(x):0.55 D(G(z)):0.45\n",
      "EPOCH [116/300] D_Loss:1.1456 G_Loss: 1.2841 D(x):0.59 D(G(z)):0.34\n",
      "EPOCH [117/300] D_Loss:1.2346 G_Loss: 0.9659 D(x):0.61 D(G(z)):0.44\n",
      "EPOCH [118/300] D_Loss:1.1967 G_Loss: 0.9671 D(x):0.59 D(G(z)):0.43\n",
      "EPOCH [119/300] D_Loss:1.1778 G_Loss: 1.1628 D(x):0.60 D(G(z)):0.38\n",
      "EPOCH [120/300] D_Loss:1.1280 G_Loss: 0.9985 D(x):0.58 D(G(z)):0.39\n",
      "EPOCH [121/300] D_Loss:1.2544 G_Loss: 1.0207 D(x):0.57 D(G(z)):0.42\n",
      "EPOCH [122/300] D_Loss:1.1970 G_Loss: 0.8181 D(x):0.59 D(G(z)):0.45\n",
      "EPOCH [123/300] D_Loss:1.2940 G_Loss: 1.0622 D(x):0.52 D(G(z)):0.39\n",
      "EPOCH [124/300] D_Loss:1.1129 G_Loss: 1.0304 D(x):0.60 D(G(z)):0.38\n",
      "EPOCH [125/300] D_Loss:1.0237 G_Loss: 1.1515 D(x):0.67 D(G(z)):0.36\n",
      "EPOCH [126/300] D_Loss:0.9169 G_Loss: 1.2041 D(x):0.69 D(G(z)):0.36\n",
      "EPOCH [127/300] D_Loss:1.3068 G_Loss: 1.2874 D(x):0.57 D(G(z)):0.38\n",
      "EPOCH [128/300] D_Loss:1.3010 G_Loss: 0.9970 D(x):0.54 D(G(z)):0.41\n",
      "EPOCH [129/300] D_Loss:1.2161 G_Loss: 1.0057 D(x):0.61 D(G(z)):0.45\n",
      "EPOCH [130/300] D_Loss:1.2723 G_Loss: 0.9625 D(x):0.52 D(G(z)):0.41\n",
      "EPOCH [131/300] D_Loss:1.3048 G_Loss: 1.2444 D(x):0.53 D(G(z)):0.37\n",
      "EPOCH [132/300] D_Loss:1.1852 G_Loss: 1.2747 D(x):0.58 D(G(z)):0.35\n",
      "EPOCH [133/300] D_Loss:0.9535 G_Loss: 1.1439 D(x):0.69 D(G(z)):0.37\n",
      "EPOCH [134/300] D_Loss:1.0430 G_Loss: 1.1845 D(x):0.63 D(G(z)):0.35\n",
      "EPOCH [135/300] D_Loss:1.1833 G_Loss: 0.8472 D(x):0.58 D(G(z)):0.43\n",
      "EPOCH [136/300] D_Loss:1.2377 G_Loss: 0.9330 D(x):0.56 D(G(z)):0.43\n",
      "EPOCH [137/300] D_Loss:1.2108 G_Loss: 0.9764 D(x):0.59 D(G(z)):0.39\n",
      "EPOCH [138/300] D_Loss:1.2414 G_Loss: 0.9032 D(x):0.58 D(G(z)):0.44\n",
      "EPOCH [139/300] D_Loss:1.3298 G_Loss: 0.8408 D(x):0.51 D(G(z)):0.44\n",
      "EPOCH [140/300] D_Loss:1.2448 G_Loss: 0.8779 D(x):0.62 D(G(z)):0.46\n",
      "EPOCH [141/300] D_Loss:1.4017 G_Loss: 1.2884 D(x):0.54 D(G(z)):0.41\n",
      "EPOCH [142/300] D_Loss:1.2284 G_Loss: 0.9539 D(x):0.59 D(G(z)):0.43\n",
      "EPOCH [143/300] D_Loss:1.3310 G_Loss: 0.8507 D(x):0.54 D(G(z)):0.43\n",
      "EPOCH [144/300] D_Loss:1.0999 G_Loss: 0.9383 D(x):0.69 D(G(z)):0.45\n",
      "EPOCH [145/300] D_Loss:1.1916 G_Loss: 0.9394 D(x):0.56 D(G(z)):0.41\n",
      "EPOCH [146/300] D_Loss:1.2490 G_Loss: 0.9631 D(x):0.57 D(G(z)):0.42\n",
      "EPOCH [147/300] D_Loss:1.2432 G_Loss: 1.0103 D(x):0.60 D(G(z)):0.42\n",
      "EPOCH [148/300] D_Loss:1.2811 G_Loss: 0.9839 D(x):0.61 D(G(z)):0.46\n",
      "EPOCH [149/300] D_Loss:1.2474 G_Loss: 0.7924 D(x):0.59 D(G(z)):0.48\n",
      "EPOCH [150/300] D_Loss:1.2607 G_Loss: 0.8652 D(x):0.55 D(G(z)):0.43\n",
      "EPOCH [151/300] D_Loss:1.2060 G_Loss: 1.0071 D(x):0.57 D(G(z)):0.40\n",
      "EPOCH [152/300] D_Loss:1.2269 G_Loss: 0.9697 D(x):0.56 D(G(z)):0.41\n",
      "EPOCH [153/300] D_Loss:1.3331 G_Loss: 0.9570 D(x):0.53 D(G(z)):0.42\n",
      "EPOCH [154/300] D_Loss:1.2903 G_Loss: 0.9675 D(x):0.55 D(G(z)):0.42\n",
      "EPOCH [155/300] D_Loss:1.3358 G_Loss: 1.0306 D(x):0.54 D(G(z)):0.41\n",
      "EPOCH [156/300] D_Loss:1.2123 G_Loss: 0.9995 D(x):0.57 D(G(z)):0.41\n",
      "EPOCH [157/300] D_Loss:1.2512 G_Loss: 1.1164 D(x):0.55 D(G(z)):0.40\n",
      "EPOCH [158/300] D_Loss:1.2284 G_Loss: 0.9974 D(x):0.58 D(G(z)):0.43\n",
      "EPOCH [159/300] D_Loss:1.3071 G_Loss: 0.8611 D(x):0.53 D(G(z)):0.44\n",
      "EPOCH [160/300] D_Loss:1.2316 G_Loss: 0.8845 D(x):0.58 D(G(z)):0.43\n",
      "EPOCH [161/300] D_Loss:1.2661 G_Loss: 0.8915 D(x):0.56 D(G(z)):0.46\n",
      "EPOCH [162/300] D_Loss:1.1505 G_Loss: 0.9581 D(x):0.58 D(G(z)):0.41\n",
      "EPOCH [163/300] D_Loss:1.1957 G_Loss: 0.8255 D(x):0.56 D(G(z)):0.43\n",
      "EPOCH [164/300] D_Loss:1.2462 G_Loss: 1.2230 D(x):0.61 D(G(z)):0.41\n",
      "EPOCH [165/300] D_Loss:1.2003 G_Loss: 0.9928 D(x):0.59 D(G(z)):0.44\n",
      "EPOCH [166/300] D_Loss:1.2731 G_Loss: 0.8942 D(x):0.53 D(G(z)):0.42\n",
      "EPOCH [167/300] D_Loss:1.2455 G_Loss: 1.0291 D(x):0.63 D(G(z)):0.44\n",
      "EPOCH [168/300] D_Loss:1.3363 G_Loss: 0.8820 D(x):0.54 D(G(z)):0.46\n",
      "EPOCH [169/300] D_Loss:1.2736 G_Loss: 0.9544 D(x):0.55 D(G(z)):0.42\n",
      "EPOCH [170/300] D_Loss:1.1748 G_Loss: 0.8428 D(x):0.58 D(G(z)):0.44\n",
      "EPOCH [171/300] D_Loss:1.2477 G_Loss: 0.9173 D(x):0.59 D(G(z)):0.45\n",
      "EPOCH [172/300] D_Loss:1.3174 G_Loss: 1.0795 D(x):0.53 D(G(z)):0.40\n",
      "EPOCH [173/300] D_Loss:1.3091 G_Loss: 0.8511 D(x):0.57 D(G(z)):0.48\n",
      "EPOCH [174/300] D_Loss:1.1827 G_Loss: 0.9666 D(x):0.56 D(G(z)):0.39\n",
      "EPOCH [175/300] D_Loss:1.0940 G_Loss: 1.0590 D(x):0.62 D(G(z)):0.39\n",
      "EPOCH [176/300] D_Loss:1.1224 G_Loss: 1.2560 D(x):0.69 D(G(z)):0.42\n",
      "EPOCH [177/300] D_Loss:1.2884 G_Loss: 1.2568 D(x):0.55 D(G(z)):0.36\n",
      "EPOCH [178/300] D_Loss:1.2843 G_Loss: 0.8836 D(x):0.55 D(G(z)):0.43\n",
      "EPOCH [179/300] D_Loss:1.0915 G_Loss: 1.1346 D(x):0.64 D(G(z)):0.38\n",
      "EPOCH [180/300] D_Loss:1.2050 G_Loss: 1.0570 D(x):0.65 D(G(z)):0.43\n",
      "EPOCH [181/300] D_Loss:1.3769 G_Loss: 0.7720 D(x):0.54 D(G(z)):0.48\n",
      "EPOCH [182/300] D_Loss:1.1486 G_Loss: 0.8964 D(x):0.61 D(G(z)):0.44\n",
      "EPOCH [183/300] D_Loss:1.3276 G_Loss: 0.9426 D(x):0.54 D(G(z)):0.43\n",
      "EPOCH [184/300] D_Loss:1.1571 G_Loss: 0.9602 D(x):0.62 D(G(z)):0.43\n",
      "EPOCH [185/300] D_Loss:1.3234 G_Loss: 0.9010 D(x):0.57 D(G(z)):0.44\n",
      "EPOCH [186/300] D_Loss:1.1922 G_Loss: 1.0176 D(x):0.56 D(G(z)):0.38\n",
      "EPOCH [187/300] D_Loss:1.1914 G_Loss: 1.0614 D(x):0.58 D(G(z)):0.38\n",
      "EPOCH [188/300] D_Loss:1.1836 G_Loss: 0.9439 D(x):0.59 D(G(z)):0.42\n",
      "EPOCH [189/300] D_Loss:1.2476 G_Loss: 1.2787 D(x):0.57 D(G(z)):0.35\n",
      "EPOCH [190/300] D_Loss:1.1979 G_Loss: 0.9939 D(x):0.58 D(G(z)):0.41\n",
      "EPOCH [191/300] D_Loss:1.2592 G_Loss: 0.9410 D(x):0.60 D(G(z)):0.45\n",
      "EPOCH [192/300] D_Loss:1.3098 G_Loss: 0.8341 D(x):0.55 D(G(z)):0.48\n",
      "EPOCH [193/300] D_Loss:1.3931 G_Loss: 0.9049 D(x):0.53 D(G(z)):0.45\n",
      "EPOCH [194/300] D_Loss:1.4289 G_Loss: 1.0025 D(x):0.49 D(G(z)):0.43\n",
      "EPOCH [195/300] D_Loss:1.2657 G_Loss: 1.0192 D(x):0.58 D(G(z)):0.43\n",
      "EPOCH [196/300] D_Loss:1.2949 G_Loss: 0.9204 D(x):0.54 D(G(z)):0.44\n",
      "EPOCH [197/300] D_Loss:1.2271 G_Loss: 0.8795 D(x):0.56 D(G(z)):0.42\n",
      "EPOCH [198/300] D_Loss:1.2842 G_Loss: 0.8288 D(x):0.55 D(G(z)):0.45\n",
      "EPOCH [199/300] D_Loss:1.3517 G_Loss: 0.9801 D(x):0.57 D(G(z)):0.45\n",
      "EPOCH [200/300] D_Loss:1.2812 G_Loss: 0.9815 D(x):0.58 D(G(z)):0.45\n",
      "EPOCH [201/300] D_Loss:1.1137 G_Loss: 1.1156 D(x):0.59 D(G(z)):0.36\n",
      "EPOCH [202/300] D_Loss:1.2036 G_Loss: 1.0005 D(x):0.58 D(G(z)):0.40\n",
      "EPOCH [203/300] D_Loss:1.2884 G_Loss: 0.9484 D(x):0.59 D(G(z)):0.45\n",
      "EPOCH [204/300] D_Loss:1.2015 G_Loss: 1.0592 D(x):0.64 D(G(z)):0.44\n",
      "EPOCH [205/300] D_Loss:1.3400 G_Loss: 0.9141 D(x):0.50 D(G(z)):0.40\n",
      "EPOCH [206/300] D_Loss:1.1056 G_Loss: 1.1540 D(x):0.63 D(G(z)):0.40\n",
      "EPOCH [207/300] D_Loss:1.2326 G_Loss: 1.0608 D(x):0.60 D(G(z)):0.41\n",
      "EPOCH [208/300] D_Loss:1.2700 G_Loss: 1.1915 D(x):0.57 D(G(z)):0.38\n",
      "EPOCH [209/300] D_Loss:1.0531 G_Loss: 1.1727 D(x):0.68 D(G(z)):0.40\n",
      "EPOCH [210/300] D_Loss:1.1274 G_Loss: 1.0096 D(x):0.61 D(G(z)):0.38\n",
      "EPOCH [211/300] D_Loss:1.0697 G_Loss: 1.2813 D(x):0.66 D(G(z)):0.35\n",
      "EPOCH [212/300] D_Loss:1.3487 G_Loss: 0.9269 D(x):0.52 D(G(z)):0.44\n",
      "EPOCH [213/300] D_Loss:1.3276 G_Loss: 0.9299 D(x):0.58 D(G(z)):0.46\n",
      "EPOCH [214/300] D_Loss:1.2493 G_Loss: 0.8753 D(x):0.62 D(G(z)):0.48\n",
      "EPOCH [215/300] D_Loss:1.2048 G_Loss: 0.8639 D(x):0.59 D(G(z)):0.45\n",
      "EPOCH [216/300] D_Loss:1.3493 G_Loss: 0.9492 D(x):0.55 D(G(z)):0.44\n",
      "EPOCH [217/300] D_Loss:1.2221 G_Loss: 1.0055 D(x):0.60 D(G(z)):0.43\n",
      "EPOCH [218/300] D_Loss:1.3103 G_Loss: 1.1812 D(x):0.57 D(G(z)):0.39\n",
      "EPOCH [219/300] D_Loss:1.2806 G_Loss: 1.0176 D(x):0.59 D(G(z)):0.44\n",
      "EPOCH [220/300] D_Loss:1.2998 G_Loss: 0.8255 D(x):0.57 D(G(z)):0.47\n",
      "EPOCH [221/300] D_Loss:1.1891 G_Loss: 0.8880 D(x):0.60 D(G(z)):0.43\n",
      "EPOCH [222/300] D_Loss:1.0544 G_Loss: 1.0025 D(x):0.68 D(G(z)):0.42\n",
      "EPOCH [223/300] D_Loss:1.2518 G_Loss: 0.8868 D(x):0.59 D(G(z)):0.45\n",
      "EPOCH [224/300] D_Loss:1.1659 G_Loss: 1.0434 D(x):0.59 D(G(z)):0.41\n",
      "EPOCH [225/300] D_Loss:1.2676 G_Loss: 0.9570 D(x):0.56 D(G(z)):0.43\n",
      "EPOCH [226/300] D_Loss:1.2342 G_Loss: 1.0315 D(x):0.55 D(G(z)):0.41\n",
      "EPOCH [227/300] D_Loss:1.3073 G_Loss: 0.8739 D(x):0.56 D(G(z)):0.46\n",
      "EPOCH [228/300] D_Loss:1.1724 G_Loss: 0.8656 D(x):0.63 D(G(z)):0.46\n",
      "EPOCH [229/300] D_Loss:1.2335 G_Loss: 1.1049 D(x):0.55 D(G(z)):0.40\n",
      "EPOCH [230/300] D_Loss:1.1939 G_Loss: 0.9317 D(x):0.62 D(G(z)):0.44\n",
      "EPOCH [231/300] D_Loss:1.3879 G_Loss: 0.9189 D(x):0.51 D(G(z)):0.43\n",
      "EPOCH [232/300] D_Loss:1.3725 G_Loss: 0.8825 D(x):0.58 D(G(z)):0.44\n",
      "EPOCH [233/300] D_Loss:1.1831 G_Loss: 0.9274 D(x):0.56 D(G(z)):0.43\n",
      "EPOCH [234/300] D_Loss:1.3191 G_Loss: 0.9677 D(x):0.53 D(G(z)):0.40\n",
      "EPOCH [235/300] D_Loss:1.3405 G_Loss: 1.0186 D(x):0.54 D(G(z)):0.42\n",
      "EPOCH [236/300] D_Loss:1.2134 G_Loss: 0.9185 D(x):0.61 D(G(z)):0.44\n",
      "EPOCH [237/300] D_Loss:1.1991 G_Loss: 0.9036 D(x):0.63 D(G(z)):0.44\n",
      "EPOCH [238/300] D_Loss:1.2454 G_Loss: 0.8273 D(x):0.58 D(G(z)):0.46\n",
      "EPOCH [239/300] D_Loss:1.2949 G_Loss: 0.8746 D(x):0.56 D(G(z)):0.44\n",
      "EPOCH [240/300] D_Loss:1.1203 G_Loss: 1.0682 D(x):0.62 D(G(z)):0.40\n",
      "EPOCH [241/300] D_Loss:1.2884 G_Loss: 0.9597 D(x):0.54 D(G(z)):0.42\n",
      "EPOCH [242/300] D_Loss:1.1555 G_Loss: 0.9061 D(x):0.61 D(G(z)):0.43\n",
      "EPOCH [243/300] D_Loss:1.3040 G_Loss: 0.9695 D(x):0.61 D(G(z)):0.46\n",
      "EPOCH [244/300] D_Loss:1.3308 G_Loss: 0.8149 D(x):0.55 D(G(z)):0.47\n",
      "EPOCH [245/300] D_Loss:1.2346 G_Loss: 0.8023 D(x):0.57 D(G(z)):0.46\n",
      "EPOCH [246/300] D_Loss:1.3353 G_Loss: 0.9961 D(x):0.54 D(G(z)):0.43\n",
      "EPOCH [247/300] D_Loss:1.3087 G_Loss: 0.9484 D(x):0.58 D(G(z)):0.45\n",
      "EPOCH [248/300] D_Loss:1.2494 G_Loss: 0.7797 D(x):0.58 D(G(z)):0.46\n",
      "EPOCH [249/300] D_Loss:1.2110 G_Loss: 0.8933 D(x):0.63 D(G(z)):0.45\n",
      "EPOCH [250/300] D_Loss:1.2118 G_Loss: 0.9426 D(x):0.62 D(G(z)):0.45\n",
      "EPOCH [251/300] D_Loss:1.1880 G_Loss: 1.0238 D(x):0.58 D(G(z)):0.42\n",
      "EPOCH [252/300] D_Loss:1.2359 G_Loss: 0.9136 D(x):0.57 D(G(z)):0.44\n",
      "EPOCH [253/300] D_Loss:1.3471 G_Loss: 0.9113 D(x):0.55 D(G(z)):0.46\n",
      "EPOCH [254/300] D_Loss:1.3419 G_Loss: 0.8107 D(x):0.53 D(G(z)):0.47\n",
      "EPOCH [255/300] D_Loss:1.3453 G_Loss: 0.8443 D(x):0.56 D(G(z)):0.49\n",
      "EPOCH [256/300] D_Loss:1.2573 G_Loss: 0.8250 D(x):0.57 D(G(z)):0.45\n",
      "EPOCH [257/300] D_Loss:1.2280 G_Loss: 0.9181 D(x):0.58 D(G(z)):0.42\n",
      "EPOCH [258/300] D_Loss:1.4057 G_Loss: 0.9491 D(x):0.54 D(G(z)):0.45\n",
      "EPOCH [259/300] D_Loss:1.0611 G_Loss: 0.9526 D(x):0.61 D(G(z)):0.39\n",
      "EPOCH [260/300] D_Loss:1.2095 G_Loss: 0.9247 D(x):0.58 D(G(z)):0.43\n",
      "EPOCH [261/300] D_Loss:1.2273 G_Loss: 0.9307 D(x):0.60 D(G(z)):0.45\n",
      "EPOCH [262/300] D_Loss:1.2163 G_Loss: 1.0330 D(x):0.57 D(G(z)):0.40\n",
      "EPOCH [263/300] D_Loss:1.3563 G_Loss: 0.9028 D(x):0.55 D(G(z)):0.44\n",
      "EPOCH [264/300] D_Loss:1.2047 G_Loss: 0.8773 D(x):0.58 D(G(z)):0.45\n",
      "EPOCH [265/300] D_Loss:1.3339 G_Loss: 0.8888 D(x):0.53 D(G(z)):0.45\n",
      "EPOCH [266/300] D_Loss:1.4180 G_Loss: 0.7858 D(x):0.56 D(G(z)):0.49\n",
      "EPOCH [267/300] D_Loss:1.3749 G_Loss: 0.9900 D(x):0.52 D(G(z)):0.42\n",
      "EPOCH [268/300] D_Loss:1.1889 G_Loss: 1.0278 D(x):0.59 D(G(z)):0.40\n",
      "EPOCH [269/300] D_Loss:1.1796 G_Loss: 0.9430 D(x):0.58 D(G(z)):0.41\n",
      "EPOCH [270/300] D_Loss:1.2361 G_Loss: 0.7934 D(x):0.59 D(G(z)):0.47\n",
      "EPOCH [271/300] D_Loss:1.2352 G_Loss: 1.0808 D(x):0.53 D(G(z)):0.39\n",
      "EPOCH [272/300] D_Loss:1.2752 G_Loss: 0.8684 D(x):0.58 D(G(z)):0.47\n",
      "EPOCH [273/300] D_Loss:1.1634 G_Loss: 1.1803 D(x):0.58 D(G(z)):0.38\n",
      "EPOCH [274/300] D_Loss:1.2773 G_Loss: 0.8194 D(x):0.56 D(G(z)):0.45\n",
      "EPOCH [275/300] D_Loss:1.2337 G_Loss: 1.0285 D(x):0.59 D(G(z)):0.42\n",
      "EPOCH [276/300] D_Loss:1.2952 G_Loss: 0.9276 D(x):0.55 D(G(z)):0.44\n",
      "EPOCH [277/300] D_Loss:1.2610 G_Loss: 0.9118 D(x):0.55 D(G(z)):0.44\n",
      "EPOCH [278/300] D_Loss:1.2465 G_Loss: 0.9915 D(x):0.58 D(G(z)):0.42\n",
      "EPOCH [279/300] D_Loss:1.2437 G_Loss: 0.8565 D(x):0.58 D(G(z)):0.44\n",
      "EPOCH [280/300] D_Loss:1.0827 G_Loss: 1.1000 D(x):0.63 D(G(z)):0.37\n",
      "EPOCH [281/300] D_Loss:1.3395 G_Loss: 0.8184 D(x):0.59 D(G(z)):0.48\n",
      "EPOCH [282/300] D_Loss:1.1723 G_Loss: 0.9603 D(x):0.57 D(G(z)):0.38\n",
      "EPOCH [283/300] D_Loss:1.2549 G_Loss: 0.9481 D(x):0.57 D(G(z)):0.43\n",
      "EPOCH [284/300] D_Loss:1.2015 G_Loss: 1.0619 D(x):0.54 D(G(z)):0.37\n",
      "EPOCH [285/300] D_Loss:1.2553 G_Loss: 0.9373 D(x):0.56 D(G(z)):0.43\n",
      "EPOCH [286/300] D_Loss:1.3390 G_Loss: 0.9431 D(x):0.54 D(G(z)):0.43\n",
      "EPOCH [287/300] D_Loss:1.2488 G_Loss: 0.8511 D(x):0.62 D(G(z)):0.47\n",
      "EPOCH [288/300] D_Loss:1.2855 G_Loss: 1.1315 D(x):0.57 D(G(z)):0.41\n",
      "EPOCH [289/300] D_Loss:1.3210 G_Loss: 0.8307 D(x):0.53 D(G(z)):0.45\n",
      "EPOCH [290/300] D_Loss:1.2639 G_Loss: 0.8234 D(x):0.55 D(G(z)):0.45\n",
      "EPOCH [291/300] D_Loss:1.1058 G_Loss: 1.4167 D(x):0.65 D(G(z)):0.36\n",
      "EPOCH [292/300] D_Loss:1.3463 G_Loss: 0.8182 D(x):0.54 D(G(z)):0.45\n",
      "EPOCH [293/300] D_Loss:1.3839 G_Loss: 0.8913 D(x):0.52 D(G(z)):0.45\n",
      "EPOCH [294/300] D_Loss:1.3037 G_Loss: 0.8364 D(x):0.54 D(G(z)):0.46\n",
      "EPOCH [295/300] D_Loss:1.2601 G_Loss: 0.9469 D(x):0.58 D(G(z)):0.43\n",
      "EPOCH [296/300] D_Loss:1.4057 G_Loss: 0.8782 D(x):0.53 D(G(z)):0.46\n",
      "EPOCH [297/300] D_Loss:1.2413 G_Loss: 0.9117 D(x):0.60 D(G(z)):0.45\n",
      "EPOCH [298/300] D_Loss:1.3856 G_Loss: 0.7639 D(x):0.57 D(G(z)):0.50\n",
      "EPOCH [299/300] D_Loss:1.2350 G_Loss: 0.8767 D(x):0.54 D(G(z)):0.43\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(BATCH_SIZE, -1).to(DEVICE)\n",
    "        \n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(DEVICE)\n",
    "        \n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = D(images, labels)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        z = torch.randn(BATCH_SIZE, 100).to(DEVICE)\n",
    "        g_label = torch.randint(0, 10, (BATCH_SIZE,)).to(DEVICE)\n",
    "        fake_images = G(z, g_label)\n",
    "        \n",
    "        outputs = D(fake_images, g_label)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        fake_images = G(z, g_label)\n",
    "        outputs = D(fake_images, g_label)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    print(\"EPOCH [{}/{}] D_Loss:{:.4f} G_Loss: {:.4f} D(x):{:.2f} D(G(z)):{:.2f}\".format(epoch, EPOCHS, d_loss.item(), g_loss.item(), real_score.mean().item(), fake_score.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "item_number = 9\n",
    "z = torch.randn(1, 100).to(DEVICE)\n",
    "g_label = torch.full((1, ), item_number, dtype = torch.long).to(DEVICE)\n",
    "sample_images = G(z, g_label)\n",
    "sample_images_img = np.reshape(sample_images.data.cpu().numpy()[0], (28, 28))\n",
    "\n",
    "plt.imshow(sample_images_img, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
